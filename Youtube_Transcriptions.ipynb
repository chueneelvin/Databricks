{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chueneelvin/Databricks/blob/main/Youtube_Transcriptions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required packages"
      ],
      "metadata": {
        "id": "ty-KHFWkGZ5D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zqSE2D64U0Fr",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d6c887c-744d-4a11-df5a-f9f9c9d26801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.16)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.16)\n",
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.10/dist-packages (0.6.2)\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.38)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.116)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m852.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_community youtube-transcript-api pytube"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the dependecies"
      ],
      "metadata": {
        "id": "qsLJSPl0GeiD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NlZpWCUdWPUO"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import YoutubeLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=aA76uj5kQac\", add_video_info=True\n",
        ")"
      ],
      "metadata": {
        "id": "pQdVJ5IjoS-f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llwsjhifoS05",
        "outputId": "5097c7c2-3811-4bb1-e001-9c49884270c6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'aA76uj5kQac', 'title': \"Build Generative AI Apps with Docker And  Hugging Face's Docker Spaces\", 'description': 'Unknown', 'view_count': 20659, 'thumbnail_url': 'https://i.ytimg.com/vi/aA76uj5kQac/hq720.jpg', 'publish_date': '2024-06-14 00:00:00', 'length': 1496, 'author': 'Krish Naik'}, page_content=\"hello all my name is krishn and welcome to my YouTube channel so guys in this specific video we are going to build a generative AI application and then with the help of Docker we are going to dockerize it and deploy it in hugging face space now what is the generative a application that uh we are going to develop here I have actually explained so we are going to probably use uh we are going to develop a text generation application using llm models and here we are also going to use Transformers along with Transformers We are going to specifically use pipeline okay so in Transformers you have this feature of pipeline uh an amazing package which actually helps you to call multiple uh llm models from Transformer itself so this entire process of creating this uh will be um you know it'll probably take around 20 to 25 minutes but again my main aim is to make sure to show you how we can probably containerize this entire generative AI application with the help of Docker and finally deploy it with in the hugging face space uh other than that if you also want to probably go ahead and deploy in other Cloud platforms like AWS and Azure you can also do that right uh but the most important thing is about dockerization how with the help of Dockers we can actually do that and if you don't know about Dockers guys I have already created an amazing one shot video I will be putting that entire Link in the uh description of this particular video where you can probably check it out complete one sh video on Dockers how to build application how to cont containerize it how to create an images everything is discussed over here but most of our work right now which we are specifically working on uh we will try to do it in the hugging face space okay so step by step first of all what we will do in our first step we will go ahead and create our generative AI application in the second step we will probably create our Docker file um while we are creating this generative AI application we also need to make sure that uh we will be using some libraries like uh fast API right which actually helps us to create a u end to end projects along with this we'll also update requirements.txt file right so requirements.txt file will also get updated with respect to all the packages that we require okay so let's go ahead and let's build this entire PL uh project and again uh if you don't have a hugging face account uh please make sure that you do that and when you go to the hugging face homepage there will be an option of something called as spaces now inside the spaces you'll be able to see that here we will be able to create our own uh deployment environment and all uh which I will be showing you after I develop the entire project so let's go ahead and let's start this specific project so guys now let's proceed towards implementing this entire project over here so uh here I've actually opened a vs code uh first of all what we are going to basically do is that go ahead and open our terminal and once we open our terminal we have to probably go ahead and create our new environment so that we'll be able to implement this specific project so first of all what I'm actually going to do I'm just going to write cont create minus P VNV python equal to here I'm actually going to use 3.9 and by default I'm going to give yes so that uh I proceed with the installation of all the basic libraries that are actually required so once this installation takes place it is probably going to take some amount of time now what I'm actually going to do over here is that let's go ahead and first of all create our requirement. txt file so quickly we will go ahead and create our requirement. txt file so requirements.txt file okay now in this requirement. txt file I'm going to update all the requirements that I specifically require so first of all we require fast API we require request we require uon uon is basically required in order to run your framework within the with the help of fast API library then we have the sentence P along with that we will also be using torch because uh the generative AI application that we are specifically going to use uh you know the models that we are going to use require Storch along with that Transformer is another Library uh inside Transformer we also have a package which is called as pipeline which will actually help you to call any models that are available in the hugging phase okay so here you can see that uh my environment has been created uh now what I can actually do is that go ahead and proceed with the installation of this all the libraries so in order to do that okay in order to do that pip install just go ahead and write minus r requirement. txt okay now see uh how we are specifically doing this right okay I did not save the requirement. txt no worries so let me just go ahead and proceed with the installation again now let this uh installation take place right uh inside my V andv environment all the libraries will get installed but uh when we are deploying in hugging face spaces right as soon as we upload this requirement. txt along with our main file that is app.py right here uh automatically the install of requirement of tstd has to be done right and for that we also going to create another Docker file so here is what we are going to use this so I have also created a Docker file over here we will go ahead go ahead and update like how step by step how we are going to probably create a container for this entire application and how uh step by step what all execution needs to happen that all will be mentioned in the docker file so in short we are going to create a package in this Docker file so one by one we will go ahead with now since our fast API is guessing uh uh you know installed right so first of all what I'm actually going to do is that let's go proceed and let's go ahead and start writing our code so as I said I'm going to create a generative AI application for text generation okay it can be a text to text generation right you can also use any other models that you want so for fast API so I'm going to import from Fast API import okay and here I'm going to basically use fast API right so this is the first library that I'm actually going to use since I've already uh in put up in the requirement. txt then from Transformers Transformers I'm going to import pipeline right we're also going to use the pipeline Library s so Transformers spelling is wrong so that is the reason I think it is giving us some errors so from Transformers import uh Transformer pipeline we are pro going to proceed we we have to use this pipeline itself right so that we'll be able to call any um llm models that are present in hugging phas okay and uh now what I'm actually going to do I'm going to go ahead and create a new Fast API API right app instance okay and here I'm going to probably create my app which I'm am going to write it as fast API okay so this is what I've actually done now um let me go ahead and create my text generation pipeline so here I'm just going to go ahead and create my text generation Pipeline and for that I will create a variable called as pipe and I will initialize this Pipeline and here I'm going to use some different different models so one of the most common model that you can probably find it out and it requires less space because see hugging face space provides you free uh free uh resources you know to deploy uh the entire application but it will be limited for you know from I think the CPU will be limited over over there the hard disk will be limited the lamb will be Ram will be limited so we will try to select Google uh let's say I will go ahead with this specific model which is called as flan flan T5 T5 small I think uh T5 small will also be there let's see flan T5 small okay so FL T5 small we will go ahead with this model because this model again it'll be requiring less space and again uh if you see this you'll be able to use this model with the help of Transformer all you have to go ahead and write this code or you can also probably start with this pipeline right so with the help of pipeline also you'll be able to call it and that is what I am planning to do it you know so I will go ahead and probably uh copy this okay and um uh I'm going to use this specific model text to text generation along with the pipeline so same code I will be pasting it over here and you can also do it from your side uh you know just copy it or use any type of model that you want okay now now I'm going to probably go ahead and create my first route which will be my homepage in my fast API and let's say I'm going to basically go ahead and write uh so this will basically be my home directory right and let's return this particular message uh saying that uh hello world or message is hello world something okay I'm just displaying some return message for the home homepage so that I'll be able to see that whether everything is working fine now I'm going to generate a fun function to handle some get request okay and this will be like SL generate okay now here what I'm actually going to do is that I'm going to create my app.get route okay get basically means your first and here I'm going to basically write generate okay so get is just for taking request right it's not post so here I'm going to probably create this particular route and let me just go ahead and uh create my generate function so here I'm going to use my generate and this text will be Str Str so whatever text I give over here it should be able to do the post to my pipe message okay and this suggestion that you'll be able to see that I'm actually getting it from uh GitHub co-pilot or uh okay I recently installed Amazon Q whisper and that is how I'm able to get it okay now uh The Next Step what I'm actually going to do over here is that I'm going to use the pipeline to generate text from given input text okay input text so I've done that and this will basically be my output which will be assigned to my pipe right the pipe that I have actually defined the pipeline okay and this will be given the text that we have given over here okay so once we get the text uh after that we can return the generated text in Json okay Json response this is how simp simple it is with the help of hugging face Transformers right and here I'm going to basically return uh so this will basically be whatever is my output so that I will give it in the form of key output colon and from the output we will be getting this particular response that we saw right it will be coming in the form of list and over there if you go ahead and see the documentation of Google flant T5 uh small it gives you the out put in a key variable which is called as generatore text okay now once you have actually done this so here what we have done is that we have created our homepage we have created our slash generate and the best thing about fast API is that it provides you it provides you uh Swagger documentation which we'll discuss about once we run this entire application in the cloud okay so uh till here I hope everybody may have understood it and we have seen that how things are basically created and uh with the help of fast API we also able to see that how a simple llm application has been generated uh which will be able to perform text to text generation okay now uh let's go back to the docker file now this is the most important thing guys because here uh inside the docker file we will we will show that how we can actually dockerize this entire application so first of all uh I will go ahead and write use the official okay since we have also created python 3.9 right python 3.9 image so for this uh what is the command that is used in Docker you have to write from python 3.9 and if you don't know about Dockers guys again I'm telling you I've created a one short video in my channel the link will be present in the description and even in the Pinn comment of this particular video okay then you can also see set the working directory to slash code okay and uh I will just write I will just go ahead and write work directory and my directory path so directory path will be uh Slash code inside this I want to probably create my working directory and then uh The Next Step will be that copy the copy the current directory contents in the container in the container at slash code the slash code path that I have actually given so for this we will use a copy command so here I just go ahead and write requirement requirements.txt and this will basically get copied to code uh SL requirements.txt okay so first of all uh we require this right requirement. txt um uh here what we are going to to do is that because this requirement. txt has all the packages and I think the spelling is correct requirement. text okay uh it has all the packages and uh here you'll be able to see that right then we will go ahead and install the requirements.txt so this is the next step we have to probably go ahead and do this uh Because unless or until the requirement. txt does not get installed uh it'll be of no use and this will basically be installed from this particular code environment right code code uh code folder so here we are writing pip install uh with no cach directory upgrade minus r requirement. txt how we install in our local right similarly pip install minus r requirement. txd but along with this we also setting up some parameters like no cache and all right then uh See since uh you know that when this image is basically created and once we host it right it is also important that we go ahead and create a user right so here uh we set up a new usern named user like run add user add user switch the user to user user right the this particular user we are just switching it okay and it is good to do this uh because uh since we are executing in a separate environment right and then finally uh we also go ahead and set the home to user home directory like initially it was SL home/ user now we going to probably set this particular path okay so environment variable we are setting it completely um this is how you basically set the environment variable and now we are going to update our working directory because inside this/ home/ user your app will be also present right so this is my working directory the new working directory that we have actually set it up and this is nothing but the user home directory right then uh one more step we really need to do is that uh we need to copy the current directory contents into the home app folder right so that is what we are going to do over here and uh let me just go ahead and use this particular command and again if you're familiar with Dockers I hope or Linux little bit of Linux you'll be able to understand it so here we are copying all the current directories this is this dot is with respect to the current directories into the Container at home/ apppp okay then finally in order to run the fastop app right start the sorry fast API app okay and here I'm going to use the port 7680 or 7860 or you can use any port that you want okay so uh for fast API you know that what are the dependencies that you required we are going to create a command prompt uon app colon app app colon app basically indicates that uh app is my file name and this is where your execution basically starts you're entering you're creating an instance of fast API right uh so that is the reason we have created over here host and uh this is uh 0.0.0 where it it needs to Pro it is the local host in whatever Cloud environment you basically run and there we going to run in the port number 7680 okay so this is my entire Docker file and uh here we have actually created it now the next step is that uh see I can I can see I already have something called as Docker desktop okay so if I go ahead and show you Docker desktop okay now I can build this entire application with the help of Dockers I can create an image over here itself and uh along with that image I can install that particular image and for that everything you require a Docker desktop so if you go ahead and search for Docker desktop right so with the help of Docker desktop you'll be able to do it right and you have for Windows Mac and Linux right uh but the only requirement with respect to using Docker desktop is that you at least need to have Windows 10 right and then you'll be able to you can see it is a out of boox conization software uh offering development and team of Robert hbri toolkit to build share and run application anyway right so all these things are there but see I have already shown you uh if you want to see my detailed one-hot video I've shown you how to install this how to start with this and all but here what we going to do is that we going to probably create our new space and we'll do all the work over here right so let me just go ahead and write text to text with Dockers okay so this is my application here I am having an option to select Docker right now now uh so many different different templates are supported by this right in the hugging phase so here you have Jupiter lab here you have a stack Auto Train and all so I will be keeping it blank now as I said right hugging face space provides you 16 GB Ram free and 2v CPU this is the basic one you also have 8 V CPU at 32 GB but for this you'll be required some charges right but here I'm going to go with this free one just to show you how you can actually do it and finally we go ahead and create the space uh let's see if any okay I have to use any license so let me just go ahead and select uh any license as such so MIT license and now let me go ahead and okay so I don't need to probably provide spaces so remove all the spaces over here and now let's click on create space now once this is done you can see you can clone this over here everything is there and right now within this particular application the file is empty right and that is what I'm actually going to do right so I'm going to open this entire file let's say reveal in file explorer and this are all my files that I want right these are all my files I require one app.py Docker do requirement. txt now what I will do I will upload it over here okay so let me just go ahead and upload it and let me copy and paste it over here right so once I upload it now you see what will happen right because already in the docker I have given all all the commands that needs to get followed right all these particular commands right so as soon as I probably put this Docker file over there and since this is already supporting that Docker environment automatically this file should run okay so now let me just go ahead and click on commit to changes now as soon as this is uploaded I will go ahead and click on app now here you can see my entire build has actually started see all Dockers uh user is adding see all the commands with respect to the docker is getting um executed one by one right and here here it is right now pushing the images exporting cache now it goes to the container right and now this container is going to probably run uh Lear uh run all these things so no module named Transformers uh let me just go ahead and see whether I have updated Transformers okay and uh I think I've done a small spelling mistake over here let me just go ahead and see app.py yes so it should be Transformers so I had done some spell mistake but it's okay okay now what I'm actually going to do again go ahead and update this app.py so let me quickly upload the file and again app do okay I don't want all the three files I'll just keep this app.py okay now as soon as I upload any of the file or I commit any changes right automatically this build process will start see this automatically the build process will start and that is how beautiful it is right now just imag imagine I've created so many videos on generative AI applications now you know if you have even paid version with respect to hugging face and all you can do all these things very much easily right now as you all know that uh my homepage should get launched so I should be able to get some kind of response as if this gets executed perfectly that is in my app.py at least I should be getting this message written message is called to hello world okay if it executes perfectly fine right so let's see how much time it is probably going to take right um and again see all the steps build has happened everything has happened we have pushed the image over here automatically this is doing now this has run see run and this is where you are getting it right here you are getting this hello world perfect right so that basically means something is working okay now the next thing is that how do I see the Swagger part right Swagger that is this SL generate and all and for that I obviously require some kind of URL so what I I'm going to do uh I'm going to click uh see in this corner right you have something over here right so here you have below this settings you have some button which is called embed the space now once you do this you can either use this as if frame so right now this entire page is basically getting used as IIM but you want a direct URL you can click on this right so here is my direct URL right and here you're getting message hello world okay now let me do one thing let me just go ahead and write slash docs if I go ahead and sl/ docs so fast API provides this entire Swagger UI documentation which we'll be able to show you in this way right all the apis like what all apis are there like slome is there right uh Slash generate now SL generate is what right if you remember what we developed in SL generate we are just going to give this particular text and this is going to probably generate the output based on the pipeline that we have created from this particular model right so this is my entire gen application now if you want to try it out right so just click on try it out and just write hey uh uh tell me tell me about machine learning okay and this is done I will just go ahead and execute it now here you should see this curl is going with all this response and uh output a machine learning algorithm is basically coming up right so uh here you can probably see a simple output yes we can go ahead and do the settings with respect to this okay so this is text to text generation right I'm giving a text and all okay uh let me just honesty is something I'll just go ahead and write this let's see what honesty is a virtue so it is completing the entire sentence so in short we're doing the text generation right and here you can see when I just write two words it is being able to give me all the other words also okay um let's say I go ahead and write more things like I can try multiple things over here um how to to be happy okay how to be a happy person see the sentence is getting completed right and this is most like a text generation concept uh where it is trying to complete the entire sentence and that is what is the power of Google fl5 right uh if you probably go ahead and see multiple examples and this is completely based on Transformers right uh and here you can see how beautifully we are able to run this using swagger UI in the fast with the help of fast API API and we are able to see that yes now everything is deployed over here so yes I hope you like this particular video I will also be giving you the link of the fast API over here hugging face space my link uh in the description of this particular video other than that go ahead and try it out and yes definitely keep on using Dockers right now every company is demanding the usage of Dockers itself so yes this was it I will see you all in the next video thank you take care have a great day bye-bye\")]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDkmlIPcb8Ml642mmvPSu+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}